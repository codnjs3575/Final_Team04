{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26323,"status":"ok","timestamp":1650469017750,"user":{"displayName":"William Yun","userId":"13312721490957957487"},"user_tz":-540},"id":"GrN-Yx46SrEe","outputId":"32a713a9-58a7-478f-fa1e-b25cb2e05568"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive/\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive/')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":39377,"status":"ok","timestamp":1650469057100,"user":{"displayName":"William Yun","userId":"13312721490957957487"},"user_tz":-540},"id":"egRldxM_WsK5","outputId":"9060bc80-ac99-483d-b63a-2d012e0ebf63"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting selenium\n","  Downloading selenium-4.1.3-py3-none-any.whl (968 kB)\n","\u001b[K     |████████████████████████████████| 968 kB 4.9 MB/s \n","\u001b[?25hCollecting urllib3[secure,socks]~=1.26\n","  Downloading urllib3-1.26.9-py2.py3-none-any.whl (138 kB)\n","\u001b[K     |████████████████████████████████| 138 kB 46.8 MB/s \n","\u001b[?25hCollecting trio~=0.17\n","  Downloading trio-0.20.0-py3-none-any.whl (359 kB)\n","\u001b[K     |████████████████████████████████| 359 kB 60.5 MB/s \n","\u001b[?25hCollecting trio-websocket~=0.9\n","  Downloading trio_websocket-0.9.2-py3-none-any.whl (16 kB)\n","Collecting sniffio\n","  Downloading sniffio-1.2.0-py3-none-any.whl (10 kB)\n","Collecting outcome\n","  Downloading outcome-1.1.0-py2.py3-none-any.whl (9.7 kB)\n","Requirement already satisfied: idna in /usr/local/lib/python3.7/dist-packages (from trio~=0.17-\u003eselenium) (2.10)\n","Requirement already satisfied: attrs\u003e=19.2.0 in /usr/local/lib/python3.7/dist-packages (from trio~=0.17-\u003eselenium) (21.4.0)\n","Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from trio~=0.17-\u003eselenium) (2.4.0)\n","Collecting async-generator\u003e=1.9\n","  Downloading async_generator-1.10-py3-none-any.whl (18 kB)\n","Collecting wsproto\u003e=0.14\n","  Downloading wsproto-1.1.0-py3-none-any.whl (24 kB)\n","Requirement already satisfied: PySocks!=1.5.7,\u003c2.0,\u003e=1.5.6 in /usr/local/lib/python3.7/dist-packages (from urllib3[secure,socks]~=1.26-\u003eselenium) (1.7.1)\n","Collecting pyOpenSSL\u003e=0.14\n","  Downloading pyOpenSSL-22.0.0-py2.py3-none-any.whl (55 kB)\n","\u001b[K     |████████████████████████████████| 55 kB 3.9 MB/s \n","\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from urllib3[secure,socks]~=1.26-\u003eselenium) (2021.10.8)\n","Collecting cryptography\u003e=1.3.4\n","  Downloading cryptography-36.0.2-cp36-abi3-manylinux_2_24_x86_64.whl (3.6 MB)\n","\u001b[K     |████████████████████████████████| 3.6 MB 48.0 MB/s \n","\u001b[?25hRequirement already satisfied: cffi\u003e=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography\u003e=1.3.4-\u003eurllib3[secure,socks]~=1.26-\u003eselenium) (1.15.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi\u003e=1.12-\u003ecryptography\u003e=1.3.4-\u003eurllib3[secure,socks]~=1.26-\u003eselenium) (2.21)\n","Collecting h11\u003c1,\u003e=0.9.0\n","  Downloading h11-0.13.0-py3-none-any.whl (58 kB)\n","\u001b[K     |████████████████████████████████| 58 kB 3.6 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from h11\u003c1,\u003e=0.9.0-\u003ewsproto\u003e=0.14-\u003etrio-websocket~=0.9-\u003eselenium) (4.1.1)\n","Installing collected packages: sniffio, outcome, h11, cryptography, async-generator, wsproto, urllib3, trio, pyOpenSSL, trio-websocket, selenium\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","requests 2.23.0 requires urllib3!=1.25.0,!=1.25.1,\u003c1.26,\u003e=1.21.1, but you have urllib3 1.26.9 which is incompatible.\n","datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n","Successfully installed async-generator-1.10 cryptography-36.0.2 h11-0.13.0 outcome-1.1.0 pyOpenSSL-22.0.0 selenium-4.1.3 sniffio-1.2.0 trio-0.20.0 trio-websocket-0.9.2 urllib3-1.26.9 wsproto-1.1.0\n","Hit:1 http://archive.ubuntu.com/ubuntu bionic InRelease\n","Get:2 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n","Ign:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n","Get:4 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n","Get:5 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n","Get:6 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n","Ign:7 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n","Get:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [696 B]\n","Hit:9 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n","Get:10 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n","Get:11 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n","Hit:12 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n","Get:13 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease [15.9 kB]\n","Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,272 kB]\n","Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [941 kB]\n","Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [3,165 kB]\n","Hit:17 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n","Get:18 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,495 kB]\n","Get:19 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,726 kB]\n","Get:20 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [907 kB]\n","Get:22 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [953 kB]\n","Get:23 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,947 kB]\n","Get:24 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [996 kB]\n","Get:25 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic/main amd64 Packages [45.3 kB]\n","Fetched 15.7 MB in 5s (3,450 kB/s)\n","Reading package lists... Done\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","The following additional packages will be installed:\n","  chromium-browser chromium-browser-l10n chromium-codecs-ffmpeg-extra\n","Suggested packages:\n","  webaccounts-chromium-extension unity-chromium-extension\n","The following NEW packages will be installed:\n","  chromium-browser chromium-browser-l10n chromium-chromedriver\n","  chromium-codecs-ffmpeg-extra\n","0 upgraded, 4 newly installed, 0 to remove and 47 not upgraded.\n","Need to get 88.3 MB of archives.\n","After this operation, 294 MB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-codecs-ffmpeg-extra amd64 99.0.4844.84-0ubuntu0.18.04.1 [1,142 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser amd64 99.0.4844.84-0ubuntu0.18.04.1 [77.7 MB]\n","Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser-l10n all 99.0.4844.84-0ubuntu0.18.04.1 [4,397 kB]\n","Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-chromedriver amd64 99.0.4844.84-0ubuntu0.18.04.1 [5,092 kB]\n","Fetched 88.3 MB in 2s (48.4 MB/s)\n","Selecting previously unselected package chromium-codecs-ffmpeg-extra.\n","(Reading database ... 155455 files and directories currently installed.)\n","Preparing to unpack .../chromium-codecs-ffmpeg-extra_99.0.4844.84-0ubuntu0.18.04.1_amd64.deb ...\n","Unpacking chromium-codecs-ffmpeg-extra (99.0.4844.84-0ubuntu0.18.04.1) ...\n","Selecting previously unselected package chromium-browser.\n","Preparing to unpack .../chromium-browser_99.0.4844.84-0ubuntu0.18.04.1_amd64.deb ...\n","Unpacking chromium-browser (99.0.4844.84-0ubuntu0.18.04.1) ...\n","Selecting previously unselected package chromium-browser-l10n.\n","Preparing to unpack .../chromium-browser-l10n_99.0.4844.84-0ubuntu0.18.04.1_all.deb ...\n","Unpacking chromium-browser-l10n (99.0.4844.84-0ubuntu0.18.04.1) ...\n","Selecting previously unselected package chromium-chromedriver.\n","Preparing to unpack .../chromium-chromedriver_99.0.4844.84-0ubuntu0.18.04.1_amd64.deb ...\n","Unpacking chromium-chromedriver (99.0.4844.84-0ubuntu0.18.04.1) ...\n","Setting up chromium-codecs-ffmpeg-extra (99.0.4844.84-0ubuntu0.18.04.1) ...\n","Setting up chromium-browser (99.0.4844.84-0ubuntu0.18.04.1) ...\n","update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n","update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n","Setting up chromium-chromedriver (99.0.4844.84-0ubuntu0.18.04.1) ...\n","Setting up chromium-browser-l10n (99.0.4844.84-0ubuntu0.18.04.1) ...\n","Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n","Processing triggers for hicolor-icon-theme (0.17-2) ...\n","Processing triggers for mime-support (3.60ubuntu1) ...\n","Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\n","/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n","\n","cp: '/usr/lib/chromium-browser/chromedriver' and '/usr/bin/chromedriver' are the same file\n"]}],"source":["#코랩에서 셀레니움 실행용\n","!pip install selenium\n","!apt-get update\n","!apt install chromium-chromedriver\n","!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n"," \n","# -*- coding: UTF-8 -*-\n","import time\n","from selenium import webdriver\n"," \n","#Colab에선 웹브라우저 창이 뜨지 않으므로 별도 설정한다.\n"," \n","options = webdriver.ChromeOptions()\n","options.add_argument('--headless')        # Head-less 설정\n","options.add_argument('--no-sandbox')\n","options.add_argument('--disable-dev-shm-usage')\n","driver = webdriver.Chrome('chromedriver', options=options)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","output_embedded_package_id":"1XfxKKGOcJSx3137aHvaG_Tj169EXzyD4"},"id":"F7g18qnHT-RT","outputId":"5c82ec5c-4bbe-429f-a5ff-02f2d199cf0a"},"outputs":[],"source":["from selenium.webdriver.chrome.service import Service\n","from bs4 import BeautifulSoup\n","import pickle\n","import requests\n","from selenium.webdriver.common.by import By\n","from selenium.webdriver.support.ui import WebDriverWait\n","from selenium.webdriver.support import expected_conditions as EC\n","from selenium.common.exceptions import TimeoutException, NoSuchElementException\n","import json\n","import csv\n","import pandas as pd\n","import glob\n","import os\n","\n","\n","data = pd.read_csv('/content/drive/MyDrive/1Colab Notebooks/last project/files/seoul_total.csv',encoding='utf-8')\n","\n","names = data['s_name']\n","adds = data['s_add']\n","\n","f_name = '/content/drive/MyDrive/1Colab Notebooks/last project/newww/seoul_addr.pkl'\n","with open(f_name, 'wb') as f:\n","  pickle.dump(adds,f)\n","\n","search_url = 'https://www.diningcode.com/list.php?query=서울%20'\n","finish_url = '\u0026rn=1'\n","\n","hrefs = []\n","count = -1\n","\n","num = 25000\n","\n","for i, name in enumerate(names[num:num+10000]):\n","    try:\n","        count +=1\n","        add = adds[num+i].split()\n","        #print(str(count)+' '+ name)\n","        url = search_url + add[1] +'%20'+ add[2]  +'%20'+ names[i] + finish_url\n","\n","        res = requests.get(url)\n","        time.sleep(1)\n","\n","        name_len = len(name) #검색할 음식점 명\n","        soup = BeautifulSoup(res.text, 'lxml')\n","        name = name.strip()\n","        name = name.replace(\" \" , \"\")\n","\n","\n","        stores = soup.find_all(\"a\", class_=\"blink\")\n","        print(str(count)+' '+name)\n","        if stores==[]:\n","            hrefs.append('')\n","        else:\n","            for store in stores:\n","                temp=''\n","                restaurants = store.find_all('span', class_='btxt')\n","\n","                for restaurant in restaurants:\n","                    temper = restaurant.get_text()   # 숫자제외 음식점명 확인\n","                    temp = temper[3:]\n","                    temp = temp.replace(\" \" , \"\")\n","                    break\n","                if temp not in name:   # 음식점명 같으면 링크 가져오기\n","                    hrefs.append('')\n","                else:\n","                    # 주소 비교하기\n","                    #print(adds[num+i][:12])\n","\n","                    addre = store.find('span', class_='ctxt').next_sibling.next_sibling\n","                    addres = addre.get_text()\n","                    #print(addres)\n","                    try:\n","                        place = store.find('i', class_='loca')\n","                        plac = place.get_text()\n","                        #print(plac)\n","                    except:\n","                        plac = ''\n","                    adr = addres.strip(plac)\n","                    #print(adr[:12])\n","                    if adds[num+i][:12] == adr[:12]:\n","                        hrefs.append(store['href'].split('rid=')[-1])  # 식당 아이디 가져옴\n","                        print('good')\n","                    else:  # 주소 다르면 다음턴으로 넘기기\n","                        hrefs.append('')\n","                        break\n","                    break\n","                break\n","                # else:  # 이름 다르면 다음턴으로 넘기기\n","                #     hrefs.append('')\n","        print(hrefs)\n","        # f_name1 = './newww/seoul1.pkl'\n","        #\n","        # with open(f_name1, 'wb') as f1:\n","        #     pickle.dump(hrefs, f1)\n","\n","\n","    except:\n","        f_name1 = '/content/drive/MyDrive/1Colab Notebooks/last project/newww/seoul'+(num+10000)+'.pkl'\n","        hrefs.append('')\n","\n","        with open(f_name1, 'wb') as f1:\n","            pickle.dump(hrefs, f1)\n","\n","        print('error occured')\n","        continue\n","\n","\n","f_name1 = '/content/drive/MyDrive/1Colab Notebooks/last project/newww/seoul'+(num+10000)+'.pkl'\n","\n","with open(f_name1, 'wb') as f1:\n","    pickle.dump(hrefs,f1)\n","\n","\n","\n","\n","\n","\n","    "]}],"metadata":{"colab":{"authorship_tag":"ABX9TyN05vTYIISLY6IW6UT+lRb7","collapsed_sections":[],"name":"crawling1st_2.ipynb","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}